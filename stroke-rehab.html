<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mixed Reality Stroke Rehabilitation - Jeremy Fischer</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="carousel.css">
</head>
<body>
    <div class="container">
        <a href="index.html" class="back-link">← Back to Portfolio</a>
        
        <div class="hero-section">
            <h1 class="hero-title">Mixed Reality Stroke Rehabilitation</h1>
            <div class="hero-subtitle">UC Berkeley | UCSF | Stanford</div>
            <div class="hero-date">March 2025 - Present</div>
            
            <!-- PROJECT CAROUSEL -->
            <div class="carousel-container" data-carousel="wargame" data-auto-advance="false" data-auto-advance-time="6000">
                <div class="carousel-wrapper">
                    <div class="carousel-slides">
                        <div class="carousel-slide square-video active">
                            <video controls muted autoplay loop>
                                <source src="images/stroke-rehab/Showcase Video.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                            <div class="caption">Exercises being performed in mixed reality</div>
                        </div>
                        <div class="carousel-slide">
                            <div class="placeholder"><img src="images/stroke-rehab/Shape Placement take 2.gif" alt=""></div>
                            <div class="caption">Remote therapy sessions give therapist a unique first person view, 3rd person virtual reconstruction, and virtual object catalog for conducting exercises.</div>
                        </div>
                        <div class="carousel-slide">
                            <div class="placeholder"><img src="images/stroke-rehab/exercise history.png" alt=""></div>
                            <div class="caption">Patient's exercise history and progress tracking interface</div>
                        </div>
                        <div class="carousel-slide">
                            <div class="placeholder"><img src="images/stroke-rehab/worksheet 2.png" alt=""></div>
                            <div class="caption">Therapists can create exercises from natural language, tweaking to the patient's ability and need, which is turned into software for headset.</div>
                        </div>
                    </div>
                    
                    <!-- Navigation arrows -->
                    <button class="carousel-btn carousel-btn-prev">
                        <span>&#8249;</span>
                    </button>
                    <button class="carousel-btn carousel-btn-next">
                        <span>&#8250;</span>
                    </button>
                </div>
                
                <!-- Slide indicators -->
                <div class="carousel-indicators">
                    <button class="indicator active"></button>
                    <button class="indicator"></button>
                    <button class="indicator"></button>
                    <button class="indicator"></button>
                </div>
            </div>
        </div>

        <div class="content-section">
            <h2 class="section-title">Project Overview</h2>
            <div class="text-content">
                <p>The Berkeley mixed reality stroke rehabilitation project is under Professor Sanjit A. Seshia's research group and partners with UCSF and Stanford hospitals. The goal of the project is to empower therapists and patients to conduct therapy sessions remotely using MR headsets like the Meta Quest 3 and create tailored exercise programs that report patient progress to therapists. This fulfills a vital need as patient waitlists for in-person therapy can last for months and many patients face locational barriers getting to clinics. My role as a researcher in the project is to lead and project manage the frontend, backend, and Unity applications.</p>
                
                <p>As project lead, I managed a team of 3-5 developers, including both student researchers and professional developers, coordinating tasks and timelines to ensure milestones were met. I designed and implemented systems across backend (Go, AWS), frontend (React, Typescript), and Unity (C#) codebases, integrating Agora video calling, Clerk authentication, AWS App runner & databases, and multi-provider AI (Gemini, OpenAI, Bedrock). I also prototyped the novel rehabilitation system using multimodal AI to generate and monitor personalized therapy exercises.</p>

                <h3>Key Features</h3>
                <ul>
                    <li>Remote therapy sessions conducted via video call between therapists on the web portal and patients on MR headset, providing therapists with a first-person view, 3D environment visualization, and real-time pose reconstruction.</li>
                    <li>Exercise programs created using natural language worksheets or recorded during therapy sessions, tailored to individual patient capabilities with automatic progress logging for therapist review.</li>
                    <li>Multimodal AI system that evaluates exercise performance using both real and virtual objects, providing automated feedback and adjustments.</li>
                    <li>End-to-end HIPAA compliant system ensuring patient data privacy and security throughout therapy sessions.</li>
                </ul>
        </div>

        <div class="content-section">
            <h2 class="section-title">Results & Future Work</h2>

            <!-- TEXT WITH IMAGE (Image on Left) -->
            <div class="text-with-image image-left">
                <div style="display: flex; gap: 30px; align-items: center; width: fit-content;">
                    <img src="images/stroke-rehab/Stanford_Cardinal_logo.svg.png" alt="Stanford Logo" style="max-height: 100px; width: auto; object-fit: contain; display: block;">
                    <img src="images/stroke-rehab/ucsf logo.png" alt="UCSF Logo" style="max-height: 100px; width: auto; object-fit: contain; display: block;">
                </div>
                <div class="text-content">
                    <p>The project has completed initial workshop prototype studies, with full patient deployment studies scheduled for Spring 2026 at UCSF and Stanford thereafter.</p>
                </div>
            </div>

            <div class="stats-grid">
                <div class="stat-item">
                    <span class="stat-number">3</span>
                    <div class="stat-label">Active Research Papers, abstract submitted to Nature Medicine, AAN</div>
                </div>
                <div class="stat-item">
                    <span class="stat-number">6+</span>
                    <div class="stat-label">Workshops Conducted, with real therapists and patients</div>
                </div>
            </div>
        </div>
        
        <!-- PARTICIPANT FEEDBACK -->
         <div class="quote-block">
            "... I liked the structured exercises and having the audio and [captions] walk me through the exercises... The menu was easy to navigate."
            <div class="quote-author">— Patient Workshop Participant</div>
        </div>
        <div class="quote-block">
            "... I see the need for something like this, I don't have anyone to drive me to therapy that can take [an hour] to get to so I stopped going."
            <div class="quote-author">— Patient Workshop Participant</div>
        </div>

        <h3 class="section-title">Collaborators</h3>
                <div class="text-content">
                    <p>
                        <strong>Sanjit A. Seshia</strong> - UC Berkeley Faculty, Principal Investigator<br>
                        <strong>Edward Kim</strong> - UC Berkeley Post Doc, Project Facilitator.<br>
                        <strong>Erik Nelson</strong> - Berkeley EECS Alumni, Head Backend Developer<br>
                        <strong>Alton Sturgis</strong> - Berkeley EECS Alumni, Unity Developer<br>
                        <strong>Junwei Lu</strong> - Berkeley EECS Student Researcher, Backend Developer<br>
                        <strong>Andrew Lau</strong> - Berkeley EECS Student Researcher, Frontend Developer<br>
                        <strong>Jaansi Parsa</strong> - Berkeley EECS, Researcher, Frontend Developer<br>
                        <strong>Lucy W</strong> - Berkeley EECS Alumni, Researcher, Frontend Developer<br>
                        <strong>Caesar Li</strong> - Berkeley EECS, Student Researcher, Unity Developer<br>
                        <strong>Yash Prakash</strong> - Software Engineer, Researcher, Unity Developer<br>
                    </p>
                </div>

        <!-- DIVIDER FOR DEEPER CONTENT -->
        <div style="border-top: 2px solid #ddd; margin: 60px 0 40px 0;"></div>

        <div class="content-section">
            <h2 class="section-title">Technical Deep Dive</h2>
            <div class="text-content">
                <h3>Remote Therapy Call</h3>
                <p>
                    The remote therapy call system leverages Agora's real-time video SDK to handle communication between therapist on the Webportal and patients using Quest 3 headset. We leveraged recently released <a href="https://developers.meta.com/horizon/blog/new-era-mixed-reality-passthrough-camera-api-machine-learning-computer-vision/"> Quest Passthrough Camera Support</a> to access front facing camera, and third-party tooling to be able to overlay virtual objects in the video stream sent to Agora. The first person view from the headset provides a unique perspective for therapists, especially occupational therapists, to assess patient's movement while completing everyday tasks. However, since it's hard to fully understand the patient's movement from just a first person view, we also provide a 3D reconstruction WebGL companion app to show a simplified patient environment and body pose estimation. The app utilizes Meta's MRUK toolkit to get object position, scale, and class info which is then synced via Photon Multiplayer. 
                </p>
                
                <!-- DOUBLE IMAGE -->
                <div class="double-image double-image-fixed">
                    <div class="image-item">
                        <img src="images/stroke-rehab/therapist view.png" alt="">
                        <div class="caption">Therapist Webportal View</div>
                    </div>
                    <div class="image-item">
                        <img src="images/stroke-rehab/patient view.png" alt="">
                        <div class="caption">Patient Headset View.</div>
                    </div>
                </div>

                <p>
                    The companion app also allows therapists to place virtual objects in the patient's environment when conducting exercises. One common use case is placing annotation shapes on tables, shelves, walls, or floors to signal to the patient where to reach, grab, place, or step. The relative positions of these virtual objects can be saved in exercise programs, tailored to the patient's physical ability and home environment.
                </p>

                <div class="single-image">
                    <img src="images/stroke-rehab/Shape Placement take 2.gif" alt="">
                    <div class="caption">Video showing how a therapist could place down virtual circle and patient seeing it in their environment.</div>
                </div>

                <h3>Exercise Programs</h3>
                <p>
                    Our system creates exercise programs with LLM assistance with natural language input from therapists. Therapists can either tweak a premade worksheet to fit a patient's needs or record exercises during therapy sessions. The input is then processed by LLMs (Claude, GPT-4, etc) to parse the natural language instructions and generate an exercise program in JSON format. Based on system prompts, the LLM chooses how each exercise step is checked for accuracy from function registry of poses (CheckIsStanding, CheckTouchingObject("Circle1"), etc) or can use image/video queries to vision models. The LLM can also call commands like setting glow hands on, spawn virtual objects, or play audio instructions during specific exercise steps.
                </p>
                <div class="double-image double-image-fixed">
                    <div class="image-item">
                        <img src="images/stroke-rehab/object placement worksheet.png" alt="">
                        <div class="caption">Natural language worksheet text describes exercise where the stroke patient's weaker arm is to place a real cup on virtual circles. Therapist can change the instructions as needed, like only having 3 targets, or using a pencil.</div>
                    </div>
                    <div class="image-item">
                        <video autoplay muted loop style="height: 400px; object-fit: contain; display: block; margin: 0 auto;">
                            <source src="images/stroke-rehab/object placement showcase.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <div class="caption" style="text-align: center;">The resulting exercise is generated and is being played.</div>
                    </div>
                </div>

                <div class="text-with-image" style="margin-top: 60px;">
                <div class="text-content">
                    <h3>Accessibility</h3>
                    <p>
                        Stroke patients often have varying levels of physical and cognitive impairments, making accessibility a top priority in our system design. We implemented multiple features to ensure ease of use for all patients. The user interface on the MR headset is designed with large, easily selectable buttons that are oriented to the non-affected side. Audio instructions are provided for each exercise step, along with visual cues like glow hands to indicate the correct hand to use. As typing in the headset can be challenging, we implemented a QR code login system where patients can scan a QR code displayed on the web portal using the headset camera to securely log in without needing to type. Stroke patients can have varying range of motion with their affected limbs, so a weekly reach calibration is done which clamps exercise object positions to within reach and can be visually seen by the therapist in web portal.  
                    </p>
                </div>
                <div class="image-container">
                    <img src="images/stroke-rehab/QR Code Scanning.gif" alt="">
                    <div class="caption">Login Token Encoded in QR Code</div>
                </div>
            </div>

            <div class="double-image double-image-fixed">
                <div class="image-item">
                    <video autoplay muted loop style="height: 400px; object-fit: contain; display: block; margin: 0 auto;">
                        <source src="images/stroke-rehab/Arm Reach Demo.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="caption" style="text-align: center;">Patient performing weekly reach calibration in headset</div>
                </div>
                <div class="image-item">
                    <img src="images/stroke-rehab/arm reach webportal.png" alt="">
                    <div class="caption">Therapist viewing patient's reach data on web portal</div>
                </div>
            </div>
            </div>
        </div>
</div>

<script src="carousel.js"></script>

</body>
</html>